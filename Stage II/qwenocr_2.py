# -*- coding: utf-8 -*-
"""qwenOCR-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nw0hl4GKdeYGDJx48b1PB6_qBXTbtzz2
"""

!pip install git+https://github.com/huggingface/transformers
!pip install qwen-vl-utils

import huggingface_hub
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    # "Qwen/Qwen2.5-VL-32B-Instruct", torch_dtype="auto", device_map="cpu"
    "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="cuda"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")

import os
import csv
from PIL import Image

image_folder = "/content/cropped_objects/content/cropped_objects"
output_csv = "ocr_results.csv"
supported_exts = (".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tiff")
ocr_results = []

resized_folder = "/tmp/resized_images"
os.makedirs(resized_folder, exist_ok=True)

for image_name in sorted(os.listdir(image_folder)):
    if not image_name.lower().endswith(supported_exts):
        continue

    original_path = os.path.join(image_folder, image_name)
    resized_path = os.path.join(resized_folder, image_name)

    # Resize image to max 720x720 while preserving aspect ratio
    with Image.open(original_path) as img:
        img = img.convert("RGB")
        img.thumbnail((720, 720), Image.Resampling.LANCZOS)
        img.save(resized_path)

    # Build messages for this single image
    messages = [
        {"role": "user",
         "content": [
            {"type": "image", "image": resized_path},
            {"type": "text", "text": "Please extract all the text from this image."},
        ]}
    ]

    # Format input
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )

    # Optional: only do this if model is fully on GPU
    inputs = inputs.to('cuda')
    model = model.to("cuda")

    # Run inference
    generated_ids = model.generate(**inputs, max_new_tokens=1024)
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]

    # Store result
    ocr_results.append((image_name, output_text.strip()))

# Print all results
#print(ocr_results)

# Write to CSV
with open(output_csv, "w", newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(["image_filename", "extracted_text"])  # header
    writer.writerows(ocr_results)

